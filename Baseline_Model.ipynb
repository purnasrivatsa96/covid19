{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import win32com.client as win32\n",
    "import pytesseract\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "#from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\puboggavarapu\\Desktop\\skills&studies\\kaggle\\covid_19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## json schema\n",
    "path = './json_schema.txt'\n",
    "with open(path) as json_data:\n",
    "    d = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save changes to git\n",
    "import os\n",
    "from subprocess import check_call\n",
    "from shlex import split\n",
    "\n",
    "def post_save( os_path):\n",
    "    \"\"\"post-save hook for doing a git commit / push\"\"\"\n",
    "    check_call(split('git add {}'.format(filename)), cwd=workdir)\n",
    "    check_call(split('git commit -m \"notebook save\" {}'.format(filename)), cwd=workdir)\n",
    "    check_call(split('git push'), cwd=workdir)\n",
    "    \n",
    "    \n",
    "file_full_path = r'C:\\Users\\puboggavarapu\\Desktop\\skills&studies\\kaggle\\github\\covid19\\Baseline_Model.ipynb'\n",
    "post_save(file_full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting full text for all files that have full text available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "custom_license        20873\n",
       "comm_use_subset        8803\n",
       "noncomm_use_subset     2133\n",
       "biorxiv_medrxiv        1020\n",
       "Name: full_text_file, dtype: int64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['full_text_file'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Middle-aged female identical twins, one of whom had systemic lupus erythematosus (SLE), were evaluated for immunologic reactivity to previous antigenic challenges, including primary immunization with a foreign antigen, keyhole limpet hemocyanin (KLH). These two women had lived together for all of their 58 years and neither was receiving anti-inflammatory or immunosuppressive drugs at the time of these studies. Both twins demonstrated comparable 7s and 19s humoral antibody response to KLH, as well as similar viral antibody titers. However, the twin with SLE was anergic to common antigens, streptokinase-streptodornase, Trichophyton and Candida; furthermore delayed hypersensitivity to KLH did not develop after immunization. This observed discrepancy between humoral and cellular immunity in genetically similar subjects may be significant in the pathogenesis of SLE.',\n",
       " 'Reports of an increased incidence of systemic lupus erythematosus (SLE), other connective tissue diseases, and serologic abnormalities in the families of oatients with SLE have empha-']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "[text['text'] for text in json.loads(open(path).read())['abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28462, 15)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_with_file = metadata[metadata.sha.notnull()]\n",
    "metadata_with_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15758, 15)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_without_file = metadata[~metadata.sha.notnull()]\n",
    "metadata_without_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For files which have json object - \n",
    "#    if full_text exists:\n",
    "#       take full text\n",
    "#    else\n",
    "#       take abstract\n",
    "texts_with_file = metadata_with_file.apply(lambda x: [text['text'] for text in json.loads(open(('./' + x['full_text_file'] + '/' + x['full_text_file'] + '/' + x['sha'].split(';')[0] + '.json')).read())['abstract']] if x['has_full_text']==False else [text['text'] for text in json.loads(open('./' + x['full_text_file'] + '/' + x['full_text_file'] + '/' + x['sha'].split(';')[0] + '.json').read())['body_text']] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paragraphs out of file-wise texts\n",
    "paras = [para for paras in texts_with_file for para in paras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-218-a207d5ea33ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Getting memory error unfortunately when trying to convert paragraph into sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\puboggavarapu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier, clean_html)\u001b[0m\n\u001b[0;32m    374\u001b[0m                                     \"get_text() function\")\n\u001b[0;32m    375\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstripped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlowerstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         _initialize_models(self, tokenizer, pos_tagger, np_extractor, analyzer,\n\u001b[0;32m    378\u001b[0m                            parser, classifier)\n",
      "\u001b[1;32mc:\\users\\puboggavarapu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\textblob\\utils.py\u001b[0m in \u001b[0;36mlowerstrip\u001b[1;34m(s, all)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mends\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \"\"\"\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_punc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Getting memory error unfortunately when trying to convert paragraph into sentences\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(\" \".join(paras))\n",
    "sentences = [item.raw for item in blob.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = './infersent/infersent1.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import InferSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = './infersent/infersent1.pkl'\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = 'glove/glove.840B.300d.txt'\n",
    "infersent.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#build vocab\n",
    "\n",
    "#infersent.build_vocab(sentences, tokenize=True)\n",
    "infersent.build_vocab(paras, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_embeddings = {}\n",
    "for i in range(len(sentences)):\n",
    "    print(i)\n",
    "    dict_embeddings[paras[i]] = infersent.encode([paras[i]], tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the paragraphs(ideally should be sentences)\n",
    "embeddings = infersent.encode(paras, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\puboggavarapu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\puboggavarapu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Pre process fucntion\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "nltk.download('wordnet')\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def preprocess(data):\n",
    "    print(1)\n",
    "    #lower case\n",
    "    text_lower = data['Text'].apply(lambda x : str(x).lower())\n",
    "    data['Text'] = text_lower\n",
    "    #remove non english terms\n",
    "    print(2)\n",
    "    data.loc[:,'Text'] = data['Text'].apply(lambda x: \" \".join(w for w in str(x).split() if d.check(w.lower())))\n",
    "    \n",
    "    #remove numbers and special characters\n",
    "    print(3)\n",
    "    data.loc[:,'Text'] = data['Text'].apply(lambda x: x.replace(r\"[^a-zA-Z ]+\", \" \").strip())\n",
    "    \n",
    "    data.loc[:,'Text'] = data['Text'].apply(lambda x:x.translate(str.maketrans('', '', string.punctuation)))\n",
    "    \n",
    "    data.loc[:,'Text'] = data['Text'].apply(lambda x: ''.join(i for i in x if not i.isdigit()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #remove punctuation\n",
    "    #data.loc[:,'Text'] = data['Text'].str.replace('[^\\w\\s]','')\n",
    "    \n",
    "    #remove sentences with less than or equal to three words\n",
    "    print(4)\n",
    "    def remove_less_than_3_word_sentences(data):\n",
    "        count = data['Text'].str.split().str.len()\n",
    "        data = data[(count>3)]\n",
    "        return data\n",
    "    data = remove_less_than_3_word_sentences(data)\n",
    "    \n",
    "    #remove stop words\n",
    "    print(5)\n",
    "    data.loc[:,'Text'] = data['Text'].apply(lambda x: \" \".join(item for item in w_tokenizer.tokenize(x) if item not in stop))\n",
    "\n",
    "    \n",
    "    #lemmatize\n",
    "    print(6)\n",
    "    data.loc[:,'Text'] = data['Text'].apply(lambda x: \" \".join(lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(str(x))))\n",
    "    \n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
